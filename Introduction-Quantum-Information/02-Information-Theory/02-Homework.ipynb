{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework of Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Callable\n",
    "from scipy.integrate import solve_ivp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(pp: np.ndarray, b: int = 2) -> float:\n",
    "    \"\"\"    \n",
    "    Calculates entropy for a discrete random variable (or joint entropies for more random variables).\n",
    "\n",
    "    Args:\n",
    "        pp: Probability vector.\n",
    "        b: Log base.\n",
    "\n",
    "    Returns:\n",
    "        float: Entropy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Probability 0 events contribute zero to the entropy\n",
    "    pp = pp[pp > 0] \n",
    "\n",
    "    # Entropy\n",
    "    return -np.sum(pp * np.log(pp) / np.log(b))\n",
    "\n",
    "\n",
    "def conditional_entropy(pp: np.ndarray, b: int = 2) -> float:\n",
    "    \"\"\"    \n",
    "    Calculates joint entropy H(X | Y) for two discrete random variables X and Y.\n",
    "\n",
    "    Args:\n",
    "        pp: Joint probability vector, which should have same Y along columns, that is pp[index_x, index_y].\n",
    "        b: Log base.\n",
    "\n",
    "    Returns:\n",
    "        float: Conditional Entropy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Marginal probability P(X)\n",
    "    ppy = np.sum(pp, axis=0, keepdims=True)\n",
    "\n",
    "    # Conditional probabiliy P(Y | X) and avoid division by zero\n",
    "    ppx_given_y = np.where(ppy > 0, pp / ppy, 0)  \n",
    "    \n",
    "    # Conditional Entropy\n",
    "    return -np.sum(pp * np.log(np.where(ppx_given_y > 0, ppx_given_y, 1)) / np.log(b))\n",
    "\n",
    "\n",
    "\n",
    "def mutual_information(pp: np.ndarray, b: int = 2) -> float:\n",
    "    \"\"\"    \n",
    "    Calculates mutual information I(X; Y) for two discrete random variables X and Y.\n",
    "\n",
    "    Args:\n",
    "        pp: Joint probability vector.\n",
    "        b: Log base.\n",
    "\n",
    "    Returns:\n",
    "        float: Mutual Information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Marginal probabilitis P(X) and P(Y)\n",
    "    ppx = np.sum(pp, axis=0, keepdims=True)\n",
    "    ppy = np.sum(pp, axis=1, keepdims=True)\n",
    "\n",
    "    # Probability 0 events contribute zero to the entropy\n",
    "    ppxy = ppx * ppy\n",
    "    ppxy = np.where(ppxy > 0, ppxy, 1)\n",
    "\n",
    "    return -np.sum(pp * np.log(ppxy/ pp) / np.log(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(pp: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"    \n",
    "    Normalizes probability vector for a set of discrete random variables.\n",
    "\n",
    "    Args:\n",
    "        pp: Input vector.\n",
    "\n",
    "    Returns:\n",
    "        float: Valid probability vector.\n",
    "    \"\"\"\n",
    "\n",
    "    return pp / np.sum(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of X: 1.561278124459133\n",
      "Entropy of Y: 1.561278124459133\n",
      "Mutual entropy of (X, Y): 3.0306390622295662\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "pp = np.array([\n",
    "    [0.1, 0.2, 0.1],\n",
    "    [0.1, 0.3, 0.2],\n",
    "    [0.2, 0.1, 0.3]\n",
    "])\n",
    "\n",
    "\n",
    "# Normalize and calculate marginal distributions\n",
    "pp = normalise(pp)\n",
    "ppx = np.sum(pp, axis = 1)\n",
    "ppy = np.sum(pp, axis = 0)\n",
    "\n",
    "\n",
    "# Determine desired properties\n",
    "print(f\"Entropy of X: {entropy(ppx)}\")\n",
    "print(f\"Entropy of Y: {entropy(ppy)}\")\n",
    "print(f\"Mutual entropy of (X, Y): {entropy(pp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "We can see that:\n",
    "- Y is the distribution with the highest entropy for two possible outcomes (total lack of knowledge, hence we assign 50% to each outcome).\n",
    "- H(X | Y) is equal to H(X) because knowing Y tells us nothing about X, as they are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint probability distribution assuming independence: \n",
      "[[0.16666667 0.16666667]\n",
      " [0.33333333 0.33333333]]\n",
      "\n",
      "Entropy of X: 0.9182958340544896\n",
      "Entropy of Y: 1.0\n",
      "Mutual entropy of (X, Y): 1.9182958340544896\n",
      "Conditional entropy H(X | Y): 0.9182958340544896\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "ppx = np.array([1/3, 2/3])\n",
    "ppy = np.array([1/2, 1/2])\n",
    "\n",
    "# Assuming independence\n",
    "pp = np.outer(ppx, ppy)\n",
    "print(f\"Joint probability distribution assuming independence: \\n{pp}\\n\")\n",
    "\n",
    "# Determine desired properties\n",
    "print(f\"Entropy of X: {entropy(ppx)}\")\n",
    "print(f\"Entropy of Y: {entropy(ppy)}\")\n",
    "print(f\"Mutual entropy of (X, Y): {entropy(pp)}\")\n",
    "print(f\"Conditional entropy H(X | Y): {conditional_entropy(pp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal probability distribution for X: [0.31578947 0.47368421 0.21052632]\n",
      "Marginal probability distribution for Y: [0.68421053 0.31578947]\n",
      "\n",
      "Probability distribution for P(X | Y): \n",
      "[[0.30769231 0.46153846 0.23076923]\n",
      " [0.33333333 0.5        0.16666667]]\n",
      "\n",
      "Probability distribution for P(Y | X): \n",
      "[[0.66666667 0.66666667 0.75      ]\n",
      " [0.33333333 0.33333333 0.25      ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "pp = np.array([\n",
    "    [0.2, 0.3, 0.15],\n",
    "    [0.1, 0.15, 0.05]\n",
    "])\n",
    "\n",
    "# Normalize and calculate marginal distributions\n",
    "pp = normalise(pp)\n",
    "ppx = np.sum(pp, axis = 0, keepdims=True)\n",
    "ppy = np.sum(pp, axis = 1, keepdims=True)\n",
    "\n",
    "print(f\"Marginal probability distribution for X: {ppx.flatten()}\")\n",
    "print(f\"Marginal probability distribution for Y: {ppy.flatten()}\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Conditional probability distributions\n",
    "ppx_given_y = pp / ppy\n",
    "ppy_given_x = pp / ppx\n",
    "\n",
    "print(f\"Probability distribution for P(X | Y): \\n{ppx_given_y}\\n\")\n",
    "print(f\"Probability distribution for P(Y | X): \\n{ppy_given_x}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint probability distribution: \n",
      "[[0.15384615 0.07692308]\n",
      " [0.30769231 0.15384615]\n",
      " [0.07692308 0.23076923]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "pp = np.array([\n",
    "    [0.1, 0.05],\n",
    "    [0.2, 0.1],\n",
    "    [0.05, 0.15]\n",
    "])\n",
    "\n",
    "# Normalize\n",
    "pp = normalise(pp)\n",
    "\n",
    "print(f\"Joint probability distribution: \\n{pp}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hypothesis H = {{(X, Y, Z) : X = 1, Y = 2, Z = 1}} has probability: 12.5%\n"
     ]
    }
   ],
   "source": [
    "# Initialize a 3D array filled with zeros\n",
    "pp = np.zeros((3, 4, 2))\n",
    "\n",
    "# Assign given probabilities\n",
    "pp[0, 0, 0] = 0.05  # P(X=1, Y=1, Z=0)\n",
    "pp[0, 1, 0] = 0.1   # P(X=1, Y=2, Z=0)\n",
    "pp[0, 1, 1] = 0.1   # P(X=1, Y=2, Z=1)\n",
    "pp[0, 2, 0] = 0.05  # P(X=1, Y=3, Z=0)\n",
    "pp[1, 0, 1] = 0.1   # P(X=2, Y=1, Z=1)\n",
    "pp[1, 1, 0] = 0.05  # P(X=2, Y=2, Z=0)\n",
    "pp[1, 2, 0] = 0.1   # P(X=2, Y=3, Z=0)\n",
    "pp[1, 3, 1] = 0.05  # P(X=2, Y=4, Z=1)\n",
    "pp[2, 0, 0] = 0.1   # P(X=3, Y=1, Z=0)\n",
    "pp[2, 2, 1] = 0.05  # P(X=3, Y=3, Z=1)\n",
    "pp[2, 3, 0] = 0.05  # P(X=3, Y=4, Z=0)\n",
    "\n",
    "# Normalize\n",
    "pp = normalise(pp)\n",
    "\n",
    "# Desired probability\n",
    "print(f\"The hypothesis H = {{{{(X, Y, Z) : X = 1, Y = 2, Z = 1}}}} has probability: {pp[0, 1, 1] * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7\n",
    "\n",
    "We have that:\n",
    "- A total of 1% of the population has the disease.\n",
    "- Hence, the probability that the person has the disease given that they have tested positive is given by Bayes's rule as (where $D$ is having the disease and $T$ is testing positive):\n",
    "    $$\n",
    "        P(D \\mid T) = \\frac{P(D \\cap T)}{P(T)}\n",
    "    $$\n",
    "\n",
    "- We can now calculate the desired probabilities:\n",
    "    - $P(D \\cap T) = P(D) \\cdot P(T) = 0.01 \\cdot 0.9 = 0.09 = 9\\%$ where independence can be assumed, has having the disease and doing a test are independent processes.\n",
    "    - $P(T) = P(D \\cap T) + P(\\bar D \\cap T) = 0.09 + 0.1 \\cdot 0.9 = 0.18 = 18\\%$\n",
    "\n",
    "Therefore, the individual has a probability of 50% of being infected! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8\n",
    "\n",
    "We have that:\n",
    "- Using Bayes's rule:\n",
    "    $$\n",
    "        P(X = 0 \\mid Y = 1) = \\frac{P(X = 0 \\cap Y = 1)}{P(Y = 1)}\n",
    "    $$\n",
    "\n",
    "- Using the BSC model we have that:\n",
    "    - $P(X = 0 \\cap Y = 1) = p \\cdot e$ because it only happens if we send a zero and there is a bit-flip.\n",
    "    - $P(Y = 1) = P(X = 0 \\cap Y = 1) + P(X = 1 \\cap Y = 1) = p \\cdot e + (1-p)\\cdot(1-e)$ has we can receive a one if a zero is sent and there is a bit flip or if a one is sent with no bit flip.\n",
    "\n",
    "Hence the desired probability is:\n",
    "$$\n",
    "    P(X = 0 \\mid Y = 1) = \\frac{p \\cdot e}{p \\cdot e + (1-p)\\cdot(1-e)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9 and Exercise 10\n",
    "\n",
    "Already solved in the statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's impossible that both balls are blue as there is only one blue ball in the box and the first ball is taken without replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12*\n",
    "\n",
    "Will do later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 13\n",
    "\n",
    "We have that:\n",
    "- The maximum likelihood estimate for X given that we have only observed X = 2 is that X = 2 with probability 1.\n",
    "- The Baysean estimate is obtained using Bayes's theorem:\n",
    "    $$\n",
    "        P(X = x \\mid X_\\text{observed} = 2) = \\frac{P(X_\\text{observed} = 2\\mid X = x)P(X = x)}{P(X_\\text{observed} = 2)}\n",
    "    $$\n",
    "\n",
    "    We have that:\n",
    "    - $P(X_\\text{observed} = 2\\mid X = x) = \\delta_{x, 2}$ ignoring measurement errors\n",
    "    - $P(X = x)$ is given by the priors\n",
    "    \n",
    "\n",
    "    Hence the Baysean estimate is the same, that is, X = 2 with probability 1 (because $P(X_\\text{observed} = 2) = P(X = 2)$ as our knowledge of the system is embedded in our prior probability):\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "        P(X = x \\mid X_\\text{observed} = 2) &= \\frac{\\delta_{x, 2}P(X = x)}{P(X_\\text{observed} = 2)}\n",
    "        \\\\\\\\\n",
    "\n",
    "        &= \\frac{\\delta_{x, 2}P(X = 2)}{P(X_\\text{observed} = 2)}\n",
    "        \\\\\\\\\n",
    "\n",
    "        &= \\delta_{x, 2}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "We can only use these methods if we have more data, if we have only one observation of the random variable the estimator just colapses to that value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 14 and Exercise 15\n",
    "\n",
    "Will do later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
