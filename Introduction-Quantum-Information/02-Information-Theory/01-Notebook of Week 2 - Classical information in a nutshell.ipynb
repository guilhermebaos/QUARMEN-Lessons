{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735561e0",
   "metadata": {},
   "source": [
    "# Week 2 - Classical information in a nutshell <a class=\"tocSkip\">\n",
    "    (c) Ariel Guerreiro 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894f867",
   "metadata": {},
   "source": [
    "In this notebook, we will dive into the fundamental concepts and techniques from classical information theory relevant to quantum information. Our focus will be on Shannon's contributions to the probabilistic formulation of information.\n",
    "\n",
    "We will begin with an introduction to the core of classical information theory. We will cover how to represent statistical ensembles and probability vectors using Python. We will also explore the probability functions of multiple variables, including joint probability distributions, conditional probability, and marginal probability.\n",
    "\n",
    "Next, we will move on to quantifying information. We will discuss Shannon entropy as a measure of uncertainty in a random variable. We will illustrate how to compress data and introduce the notions of relative entropy, joint entropy, conditional information, and mutual information as relative measures of information.\n",
    "\n",
    "We will then discuss information channels as paradigms of information systems. We will cover noisy channels, error correction, and the capacity of a channel.\n",
    "\n",
    "Finally, we will conclude with a discussion of inference as a way to extract information about one random variable from another.\n",
    "\n",
    "Throughout the notebook, we will provide Python codes to illustrate the concepts, examples, and exercises, both computational and analytical. The goal is to promote a hands-on approach to the study of classical information theory and its relationship to quantum information.\n",
    "\n",
    "To get started, we first need to import the necessary Python libraries and use the magic lines to get your notebook ready. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a065cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:20.980879Z",
     "start_time": "2023-02-25T13:37:19.379368Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import random\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b60aa",
   "metadata": {},
   "source": [
    "## The core in the shell of classical information theory\n",
    "\n",
    "This section provides the foundation for understanding classical information theory. It introduces the basics of probability theory and how it is used to describe classical information.\n",
    "\n",
    "We will start by discussing the representation of statistical ensembles and probability vectors using Python. We will explore how to model the probability distribution of a random variable and how to describe the probability functions of multiple variables.\n",
    "\n",
    "We will then introduce the concepts of joint probability distributions, conditional probability, and marginal probability. Joint probability distributions allow us to analyze the relationship between two or more random variables, while conditional probability and marginal probability help us to extract information about one variable from another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b415b0f",
   "metadata": {},
   "source": [
    "### Information as a Probability Distribution\n",
    "In classical information theory, the uncertainty or randomness associated with an event is characterized by its probability distribution over the set of possible states. A probability distribution is a function that describes the likelihood of each possible outcome in a random event. It can be represented by a probability mass function (PMF), which maps each outcome to a probability value.\n",
    "\n",
    "#### Ensembles and Probability Distribution\n",
    "An ensemble in probability and classical information theory is a mathematical representation of a random variable and its associated probabilities. It consists of three components: an outcome $x$, a set of possible values $\\Omega_X$, and a probability distribution $P_X$. The outcome $x$ refers to the values taken by a random variable $X$. The set of possible values $\\Omega_X$ is a list of all the possible outcomes for the random variable $X$. The probability distribution $P_X$ is an array of values, each representing the probability of a particular outcome.\n",
    "\n",
    "In an ensemble, a probability distribution is associated with a specific random variable, $X$. For example, if we consider the random variable $X$ to be the result of a coin flip, then the possible outcomes are \"heads\" and \"tails\" and the associated probabilities are 0.5 each. An ensemble for this scenario can be defined as:\n",
    "\n",
    "$$ (x, \\Omega_X, P_X) = (\"heads\",{\"heads\",\"tails\"},{0.5,0.5})$$\n",
    "\n",
    "Here, $x = \\text{\"heads\"}$ is the outcome, $\\Omega_X = \\{\\text{\"heads\"}, \\text{\"tails\"}\\}$ is the set of possible values, and $P_X = \\{0.5, 0.5\\}$ is the probability distribution.\n",
    "\n",
    "Here is a simple implementation of an ensemble class in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9663ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:20.996610Z",
     "start_time": "2023-02-25T13:37:20.983425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome: heads\n",
      "Set of possible values: ['heads', 'tails']\n",
      "Probability distribution: [0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "class Ensemble:\n",
    "    def __init__(self, x, O_X, P_X):\n",
    "        self.x = x\n",
    "        self.O_X = O_X\n",
    "        self.P_X = P_X\n",
    "        \n",
    "    def display(self):\n",
    "        print(\"Outcome:\", self.x)\n",
    "        print(\"Set of possible values:\", self.O_X)\n",
    "        print(\"Probability distribution:\", self.P_X)\n",
    "        \n",
    "ensemble = Ensemble(\"heads\", [\"heads\", \"tails\"], [0.5, 0.5])\n",
    "ensemble.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b499ee",
   "metadata": {},
   "source": [
    "#### Probability Vector\n",
    "\n",
    "Working throughout this lecture course with Ensembles, specially in the python encoding, is ..... not as easy and clean at it may appear at first glance (we would need to use classes and encapsulation). Hence we adopt another approach which is basically splitting the notion of ensemble in its three parts. \n",
    "\n",
    "The information about a random variable $X$ can also be described by a function $p(x)$, which maps each state $x$ of $X$ to a non-negative real value $p(x)$, such that the sum of all probabilities of observing the corresponding states of $X$, $\\sum_x p(x) = 1$. This is known as a probability vector, which can be represented as a 1-dimensional numpy array. The normalization property ensures that the sum of all values in the array is equal to 1, implying that the event is certain to occur.\n",
    "\n",
    "Consider the previous example, where we were modeling the outcome of a fair coin flip as a random variable $X$ with two possible values: \"heads\" and \"tails\". The associated probabilities of each outcome can be represented as a probability vector $p = [p_1, p_2]$, where $p_1$ and $p_2$ are the probabilities of observing \"heads\" and \"tails\", respectively.\n",
    "\n",
    "For a fair coin flip, both \"heads\" and \"tails\" have an equal probability of 0.5, so we have $p = [0.5, 0.5]$. To implement this in Python, we can define the probability vector as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6e9dcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.012595Z",
     "start_time": "2023-02-25T13:37:20.999591Z"
    }
   },
   "outputs": [],
   "source": [
    "p = [0.5, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cfe593",
   "metadata": {},
   "source": [
    "We can access each element of the vector to retrieve the associated probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c43e21a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.028514Z",
     "start_time": "2023-02-25T13:37:21.015548Z"
    }
   },
   "outputs": [],
   "source": [
    "p_heads = p[0]\n",
    "p_tails = p[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9ccd2",
   "metadata": {},
   "source": [
    "#### Outcome set\n",
    "\n",
    "Another component of the ensemble is the outcome set defined as the set of all possible outcomes, denoted by $\\Omega_X$.\n",
    "\n",
    "In Python a way to define a outcome set is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf176158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.043511Z",
     "start_time": "2023-02-25T13:37:21.030509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of heads is: 0.5\n"
     ]
    }
   ],
   "source": [
    "O = ('heads','tails')\n",
    "print('The probability of ' + O[0] + ' is: ' + str(p[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f885c",
   "metadata": {},
   "source": [
    "Another way of representing the distribution is via a probability mass function using a python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "427fd92e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.059469Z",
     "start_time": "2023-02-25T13:37:21.045508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "pmf = {\n",
    "    ('heads'): 0.5,\n",
    "    ('rails'): 0.5\n",
    "}\n",
    "print(pmf.get('heads', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1213e2b9",
   "metadata": {},
   "source": [
    "These structures contain information about the outcome set  $\\Omega_X$ and probabilities. The only thing lacking is the random variable which is implicit.\n",
    "\n",
    "So we have seen developing ways of developing python objects that work as containers for ensembles and probability vectors. They should be used as needed. Finally, we should stress that if may be possible to use a Numpy array to express probability vectors whenever we are going to use calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e39571",
   "metadata": {},
   "source": [
    "#### Basic probability theory (Kolmogorov style)\n",
    "\n",
    "In probability theory, we often talk about events, which are subsets of the outcome set $\\Omega_X$.\n",
    "For example, if we're rolling a six-sided die, then $\\Omega_X = {1, 2, 3, 4, 5, 6}$, and the event \"rolling an even number\" is the subset $A = {2, 4, 6}$. Notice that you can think of $A$ as an hypothesis, as described in the lecture notes.\n",
    "\n",
    "\n",
    "Probability is a way of assigning a numerical value to events. The probability of an event $A$, denoted by $P(A)$, is a number between 0 and 1 that reflects the likelihood of the event occurring. A probability of 0 means that the event is impossible, while a probability of 1 means that the event is certain. For example, the probability of rolling an even number on a six-sided die is $P({2, 4, 6}) = \\frac{1}{2} = 0.5$.\n",
    "\n",
    "To calculate probabilities, we use probability axioms and rules. These include the basic axioms of probability, such as:\n",
    "\n",
    "* $P(A) \\geq 0$ for any event $A$;\n",
    "* $P(\\Omega_X) = 1$;\n",
    "* If $A$ and $B$ are disjoint (i.e., have no outcomes in common), then $P(A \\cup B) = P(A) + P(B)$, and $P(A \\cup B) = P(A) + P(B) -P(A\\cap B)$\n",
    "\n",
    "There are also more advanced rules, such as the law of total probability and Bayes' theorem, which are used to calculate conditional probabilities and make predictions based on uncertain information.\n",
    "\n",
    "The following code generates a Venn diagram of two events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3176476f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.487311Z",
     "start_time": "2023-02-25T13:37:21.061463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"308.798522pt\" height=\"232.724739pt\" viewBox=\"0 0 308.798522 232.724739\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-02-25T13:37:21.452404</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 232.724739 \n",
       "L 308.798522 232.724739 \n",
       "L 308.798522 0 \n",
       "L 0 0 \n",
       "L 0 232.724739 \n",
       "z\n",
       "\" style=\"fill: none\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 134.79731 184.295465 \n",
       "C 124.882866 175.873688 116.917762 165.395417 111.45574 153.589112 \n",
       "C 105.993718 141.782807 103.164357 128.928556 103.164357 115.92 \n",
       "C 103.164357 102.911444 105.993718 90.057193 111.45574 78.250888 \n",
       "C 116.917762 66.444583 124.882866 55.966312 134.79731 47.544535 \n",
       "C 123.239898 42.005607 110.464622 39.486357 97.669673 40.223063 \n",
       "C 84.874723 40.959769 72.47302 44.928658 61.627718 51.757486 \n",
       "C 50.782416 58.586315 41.843516 68.054702 35.649397 79.274611 \n",
       "C 29.455277 90.49452 26.205834 103.103859 26.205834 115.92 \n",
       "C 26.205834 128.736141 29.455277 141.34548 35.649397 152.565389 \n",
       "C 41.843516 163.785298 50.782416 173.253685 61.627718 180.082514 \n",
       "C 72.47302 186.911342 84.874723 190.880231 97.669673 191.616937 \n",
       "C 110.464622 192.353643 123.239898 189.834393 134.79731 184.295465 \n",
       "\" clip-path=\"url(#pdae13a8a41)\" style=\"fill: #ff0000; opacity: 0.4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 134.79731 47.544535 \n",
       "C 147.688637 53.722746 158.573669 63.422546 166.190657 75.519575 \n",
       "C 173.807645 87.616604 177.850452 101.624664 177.850452 115.92 \n",
       "C 177.850452 130.215336 173.807645 144.223396 166.190657 156.320425 \n",
       "C 158.573669 168.417454 147.688637 178.117254 134.79731 184.295465 \n",
       "C 147.846688 195.380197 163.800588 202.500961 180.765654 204.812688 \n",
       "C 197.73072 207.124415 215.008191 204.531889 230.547634 197.342783 \n",
       "C 246.087078 190.153676 259.248451 178.664097 268.469807 164.237588 \n",
       "C 277.691162 149.811079 282.592688 133.041844 282.592688 115.92 \n",
       "C 282.592688 98.798156 277.691162 82.028921 268.469807 67.602412 \n",
       "C 259.248451 53.175903 246.087078 41.686324 230.547634 34.497217 \n",
       "C 215.008191 27.308111 197.73072 24.715585 180.765654 27.027312 \n",
       "C 163.800588 29.339039 147.846688 36.459803 134.79731 47.544535 \n",
       "\" clip-path=\"url(#pdae13a8a41)\" style=\"fill: #008000; opacity: 0.4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 134.79731 184.295465 \n",
       "C 147.688637 178.117254 158.573669 168.417454 166.190657 156.320425 \n",
       "C 173.807645 144.223396 177.850452 130.215336 177.850452 115.92 \n",
       "C 177.850452 101.624664 173.807645 87.616604 166.190657 75.519575 \n",
       "C 158.573669 63.422546 147.688637 53.722746 134.79731 47.544535 \n",
       "C 124.882866 55.966312 116.917762 66.444583 111.45574 78.250888 \n",
       "C 105.993718 90.057193 103.164357 102.911444 103.164357 115.92 \n",
       "C 103.164357 128.928556 105.993718 141.782807 111.45574 153.589112 \n",
       "C 116.917762 165.395417 124.882866 175.873688 134.79731 184.295465 \n",
       "\" clip-path=\"url(#pdae13a8a41)\" style=\"fill: #b25900; opacity: 0.4\"/>\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    <!-- 3 -->\n",
       "    <g transform=\"translate(61.503846 118.679375)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_2\">\n",
       "    <!-- 5 -->\n",
       "    <g transform=\"translate(227.04032 118.679375)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_3\">\n",
       "    <!-- 2 -->\n",
       "    <g transform=\"translate(137.326154 118.679375)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_4\">\n",
       "    <!-- A -->\n",
       "    <g transform=\"translate(93.819393 209.137258)scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-41\" d=\"M 2188 4044 \n",
       "L 1331 1722 \n",
       "L 3047 1722 \n",
       "L 2188 4044 \n",
       "z\n",
       "M 1831 4666 \n",
       "L 2547 4666 \n",
       "L 4325 0 \n",
       "L 3669 0 \n",
       "L 3244 1197 \n",
       "L 1141 1197 \n",
       "L 716 0 \n",
       "L 50 0 \n",
       "L 1831 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-41\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_5\">\n",
       "    <!-- B -->\n",
       "    <g transform=\"translate(192.878522 223.029114)scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-42\" d=\"M 1259 2228 \n",
       "L 1259 519 \n",
       "L 2272 519 \n",
       "Q 2781 519 3026 730 \n",
       "Q 3272 941 3272 1375 \n",
       "Q 3272 1813 3026 2020 \n",
       "Q 2781 2228 2272 2228 \n",
       "L 1259 2228 \n",
       "z\n",
       "M 1259 4147 \n",
       "L 1259 2741 \n",
       "L 2194 2741 \n",
       "Q 2656 2741 2882 2914 \n",
       "Q 3109 3088 3109 3444 \n",
       "Q 3109 3797 2882 3972 \n",
       "Q 2656 4147 2194 4147 \n",
       "L 1259 4147 \n",
       "z\n",
       "M 628 4666 \n",
       "L 2241 4666 \n",
       "Q 2963 4666 3353 4366 \n",
       "Q 3744 4066 3744 3513 \n",
       "Q 3744 3084 3544 2831 \n",
       "Q 3344 2578 2956 2516 \n",
       "Q 3422 2416 3680 2098 \n",
       "Q 3938 1781 3938 1306 \n",
       "Q 3938 681 3513 340 \n",
       "Q 3088 0 2303 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-42\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_6\">\n",
       "    <!-- $P(A)$ -->\n",
       "    <g transform=\"translate(40.989192 87.411249)scale(0.14 -0.14)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-Oblique-50\" d=\"M 1081 4666 \n",
       "L 2541 4666 \n",
       "Q 3178 4666 3512 4369 \n",
       "Q 3847 4072 3847 3500 \n",
       "Q 3847 2731 3353 2303 \n",
       "Q 2859 1875 1966 1875 \n",
       "L 1172 1875 \n",
       "L 806 0 \n",
       "L 172 0 \n",
       "L 1081 4666 \n",
       "z\n",
       "M 1613 4147 \n",
       "L 1275 2394 \n",
       "L 2069 2394 \n",
       "Q 2606 2394 2893 2669 \n",
       "Q 3181 2944 3181 3456 \n",
       "Q 3181 3784 2986 3965 \n",
       "Q 2791 4147 2438 4147 \n",
       "L 1613 4147 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-Oblique-41\" d=\"M 2356 4666 \n",
       "L 3072 4666 \n",
       "L 3938 0 \n",
       "L 3278 0 \n",
       "L 3084 1197 \n",
       "L 984 1197 \n",
       "L 325 0 \n",
       "L -341 0 \n",
       "L 2356 4666 \n",
       "z\n",
       "M 2584 4044 \n",
       "L 1275 1722 \n",
       "L 2988 1722 \n",
       "L 2584 4044 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-Oblique-50\" transform=\"translate(0 0.125)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(60.302734 0.125)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-Oblique-41\" transform=\"translate(99.316406 0.125)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(167.724609 0.125)\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_7\">\n",
       "    <!-- $P(B)$ -->\n",
       "    <g transform=\"translate(231.047535 87.411249)scale(0.14 -0.14)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-Oblique-42\" d=\"M 1081 4666 \n",
       "L 2694 4666 \n",
       "Q 3350 4666 3675 4422 \n",
       "Q 4000 4178 4000 3688 \n",
       "Q 4000 3238 3720 2911 \n",
       "Q 3441 2584 2988 2516 \n",
       "Q 3375 2428 3569 2181 \n",
       "Q 3763 1934 3763 1522 \n",
       "Q 3763 819 3242 409 \n",
       "Q 2722 0 1819 0 \n",
       "L 172 0 \n",
       "L 1081 4666 \n",
       "z\n",
       "M 1234 2228 \n",
       "L 903 519 \n",
       "L 1919 519 \n",
       "Q 2491 519 2800 781 \n",
       "Q 3109 1044 3109 1522 \n",
       "Q 3109 1891 2904 2059 \n",
       "Q 2700 2228 2247 2228 \n",
       "L 1234 2228 \n",
       "z\n",
       "M 1606 4147 \n",
       "L 1331 2741 \n",
       "L 2272 2741 \n",
       "Q 2775 2741 3058 2959 \n",
       "Q 3341 3178 3341 3566 \n",
       "Q 3341 3869 3150 4008 \n",
       "Q 2959 4147 2541 4147 \n",
       "L 1606 4147 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-Oblique-50\" transform=\"translate(0 0.125)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(60.302734 0.125)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-Oblique-42\" transform=\"translate(99.316406 0.125)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(167.919922 0.125)\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_8\">\n",
       "    <!-- $P(A\\cap B)$ -->\n",
       "    <g transform=\"translate(112.261071 87.411249)scale(0.14 -0.14)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-2229\" d=\"M 825 1850 \n",
       "Q 825 2775 1208 3240 \n",
       "Q 1591 3706 2344 3706 \n",
       "Q 3094 3706 3476 3240 \n",
       "Q 3859 2775 3859 1850 \n",
       "L 3859 0 \n",
       "L 3322 0 \n",
       "L 3322 1772 \n",
       "Q 3322 2534 3090 2864 \n",
       "Q 2859 3194 2344 3194 \n",
       "Q 1825 3194 1594 2864 \n",
       "Q 1363 2534 1363 1772 \n",
       "L 1363 0 \n",
       "L 825 0 \n",
       "L 825 1850 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-Oblique-50\" transform=\"translate(0 0.125)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(60.302734 0.125)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-Oblique-41\" transform=\"translate(99.316406 0.125)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-2229\" transform=\"translate(187.207031 0.125)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-Oblique-42\" transform=\"translate(279.882812 0.125)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(348.486328 0.125)\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_9\">\n",
       "    <!-- $\\Omega$ -->\n",
       "    <g transform=\"translate(155.024198 20.890828)scale(0.14 -0.14)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-3a9\" d=\"M 4647 556 \n",
       "L 4647 0 \n",
       "L 2772 0 \n",
       "L 2772 556 \n",
       "Q 3325 859 3634 1378 \n",
       "Q 3944 1897 3944 2528 \n",
       "Q 3944 3278 3531 3731 \n",
       "Q 3119 4184 2444 4184 \n",
       "Q 1769 4184 1355 3729 \n",
       "Q 941 3275 941 2528 \n",
       "Q 941 1897 1250 1378 \n",
       "Q 1563 859 2119 556 \n",
       "L 2119 0 \n",
       "L 244 0 \n",
       "L 244 556 \n",
       "L 1241 556 \n",
       "Q 747 991 519 1456 \n",
       "Q 294 1922 294 2497 \n",
       "Q 294 3491 894 4106 \n",
       "Q 1491 4722 2444 4722 \n",
       "Q 3391 4722 3994 4106 \n",
       "Q 4594 3494 4594 2528 \n",
       "Q 4594 1922 4372 1459 \n",
       "Q 4150 997 3647 556 \n",
       "L 4647 556 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-3a9\" transform=\"translate(0 0.21875)\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pdae13a8a41\">\n",
       "   <rect x=\"7.2\" y=\"7.2\" width=\"294.398522\" height=\"217.44\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib_venn import venn2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set sizes\n",
    "A_size = 3\n",
    "B_size = 5\n",
    "AB_size = 2\n",
    "\n",
    "# Draw Venn diagram\n",
    "venn2(subsets=(A_size, B_size, AB_size), set_labels=('A', 'B'))\n",
    "\n",
    "# Add labels\n",
    "plt.annotate('$P(A)$', xy=(-0.6, 0.15), fontsize=14)\n",
    "plt.annotate('$P(B)$', xy=(0.4, 0.15), fontsize=14)\n",
    "plt.annotate('$P(A\\\\cap B)$', xy=(-0.225, 0.15), fontsize=14)\n",
    "plt.annotate('$\\Omega$', xy=(0, 0.5), fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3901114",
   "metadata": {},
   "source": [
    "The diagram shows two sets, A and B, and their intersection, which is labeled as $A\\cap B$. The area of each set corresponds to the probability of each event. For example, the area of set A represents the probability of event A, denoted by $P(A)$. Similarly, the area of set B represents the probability of event B, denoted by $P(B)$. The intersection of A and B, labeled $A\\cap B$, represents the probability of the joint event \"A and B\", denoted by $P(A\\cap B)$. The diagram illustrates the  axioms of probability.\n",
    "\n",
    "\n",
    "\n",
    "The areas of sets A and B cannot be negative, so the diagram satisfies the non-negativity axiom. The total area of the diagram corresponds to the sample space $\\Omega$, and is equal to 1, which satisfies the normalization axiom.\n",
    "\n",
    "Also, the graphical representation allows to illustrate the third axiom: $P(A \\cup B) = P(A) + P(B) -P(A\\cap B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9823cf9",
   "metadata": {},
   "source": [
    "### Information with multiple variables\n",
    "\n",
    "#### Probability Distributions\n",
    "\n",
    "In many cases, we are interested in events involving multiple random variables, such as the joint distribution of two random variables X and Y. A joint probability distribution is used to represent the probability of each possible combination of outcomes. This joint probability distribution is a function $p(x,y)$ that maps each pair of states $(x,y)$ to a non-negative real value $p(x,y)$, such that the sum over all possible pairs $(x,y)$ equals 1:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{x,y} p(x,y) = 1\n",
    "\\end{equation*}\n",
    "\n",
    "The joint probability distribution can be represented as a 2-dimensional numpy array, where each row corresponds to a possible value of X and each column corresponds to a possible value of Y.\n",
    "\n",
    "By using probability distributions, classical information theory provides a way to quantify and analyze the amount of uncertainty or randomness associated with a certain event. This concept is fundamental to many applications in fields such as communication, data compression, and information processing.\n",
    "\n",
    "#### Conditional Probability\n",
    "\n",
    "Besides the joint probability distribution, there are other forms of probability functions that are applied to sets of two or more variables. One important example of such probability function is conditional probability.\n",
    "\n",
    "The Conditional Probability of an event A given another event B, denoted by $P(A|B)$, is defined as the probability of event A happening given that event B has already happened. Mathematically, it can be expressed as:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "\\end{equation*}\n",
    "\n",
    "where $P(A\\cap B)$ is the probability of both events $A$ and $B$ happening at the same time and $P(B)$ is the probability of event $B$ happening.\n",
    "\n",
    "The conditional probability is useful when we are interested in understanding the relationship between two events and how the occurrence of one event affects the probability of another event happening.\n",
    "\n",
    "For example, suppose a box  contains three types of balls: red, blue, and green. Let R, B, and G denote the events that a ball drawn from the box is red, blue, and green, respectively. Suppose that the probability of drawing a red ball is 0.4, the probability of drawing a blue ball is 0.3, and the probability of drawing a green ball is 0.3.\n",
    "\n",
    "If you draw a ball from the box and it is not blue, you can use the formula for conditional probability to calculate the probability of drawing a red ball given that the ball drawn is not blue:\n",
    "\n",
    "$$P(R|\\bar{B}) = \\frac{P(R \\cap \\bar{B})}{P(\\bar{B})}$$\n",
    "\n",
    "where $\\bar{B}$ denotes the complement of event $B$ (i.e., the event that a ball drawn is not blue).\n",
    "\n",
    "To apply the formula, we need to calculate the joint probability of drawing a red ball and not drawing a blue ball, as well as the probability of not drawing a blue ball. The joint probability of drawing a red ball and not drawing a blue ball is simply the probability of drawing a red ball, because if the ball drawn is red, then it cannot be blue. Thus, we have:\n",
    "\n",
    "$$P(R \\cap \\bar{B}) = P(R) = 0.4$$\n",
    "\n",
    "To calculate the probability of not drawing a blue ball, we can use the law of total probability, which states that the probability of an event can be calculated by summing over all possible ways in which the event can occur. In this case, we can partition the event \"drawing a ball that is not blue\" into three mutually exclusive events: drawing a red ball, drawing a green ball, or not drawing a red, blue, or green ball (i.e., drawing an \"other\" ball). Thus, we have:\n",
    "\n",
    "$$P(\\bar{B}) = P(R \\cap \\bar{B}) + P(G \\cap \\bar{B}) + P(O \\cap \\bar{B})$$\n",
    "\n",
    "where O denotes the event of drawing an \"other\" ball. We can calculate the probabilities of these three events as follows:\n",
    "\n",
    "$$P(G \\cap \\bar{B}) = P(G) \\times P(\\bar{B} | G) = 0.3 \\times 1 = 0.3$$\n",
    "\n",
    "because if the ball drawn is green, then it cannot be blue. Similarly, we have:\n",
    "\n",
    "$$P(O \\cap \\bar{B}) = P(O) \\times P(\\bar{B} | O) = 0 $$\n",
    "\n",
    "because if the ball drawn is not red, blue, or green, then it cannot be blue. Thus, we have:\n",
    "\n",
    "$$P(\\bar{B}) = 0.4 + 0.3 + 0= 0.7$$\n",
    "\n",
    "Substituting these values into the formula for conditional probability, we obtain:\n",
    "\n",
    "$$P(R|\\bar{B}) = \\frac{P(R \\cap \\bar{B})}{P(\\bar{B})} = \\frac{0.4}{0.7} \\approx 0.57$$\n",
    "\n",
    "So the probability of drawing a red ball given that the ball drawn is not blue, in the case of a box with three colors of balls, is 0.57.\n",
    "\n",
    "This concept of conditional probability is important because it allows us to make predictions about events based on prior knowledge or information. For instance, in the case of Bayesian inference, the conditional probability distribution is used to make predictions about an unknown variable given some observed data.\n",
    "\n",
    "Conditional probability, on the other hand, is the likelihood of an event occurring given that another event has already occurred. It can be calculated using Bayes' theorem, which relates the conditional probability to the joint probability and the individual probabilities of the events. The formula for calculating conditional probability is $P(A|B) = P(A \\cap B) / P(B)$, where $P(A|B)$ represents the probability of event A given that event B has occurred, $P(A \\cap B)$ represents the joint probability of A and B occurring, and $P(B)$ represents the probability of event B occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7d42e",
   "metadata": {},
   "source": [
    "For events involving multiple random variables, such as the joint distribution of two random variables X and Y, a joint probability distribution is used to represent the probability of each possible combination of outcomes. This joint probability distribution is a function $p(x,y)$ that maps each pair of states $(x,y)$ to a non-negative real value $p(x,y)$, such that the sum over all possible pairs $(x,y)$ equals 1: $$\\sum_{x,y} p(x,y) = 1.$$ \n",
    "The joint probability distribution can be represented as a 2-dimensional numpy array, where each row corresponds to a possible value of X and each column corresponds to a possible value of Y.\n",
    "\n",
    "By using probability distributions, classical information theory provides a way to quantify and analyze the amount of uncertainty or randomness associated with a certain event. This concept is fundamental to many applications in fields such as communication, data compression, and information processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022553a7",
   "metadata": {},
   "source": [
    "In classical information theory, besides the joint probability distribution, there are other forms of probability functions that are applied to sets of two or more variables. Two important examples of such probability functions are conditional probability and marginal probability.\n",
    "\n",
    "The **Conditional Probability** of an event $A$ given another event $B$, denoted by $P(A|B)$, is defined as the probability of event $A$ happening given that event $B$ has already happened. Mathematically, it can be expressed as:\n",
    "\n",
    "$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$,\n",
    "\n",
    "where $P(A \\cap B)$ is the probability of both events $A$ and $B$ happening at the same time and $P(B)$ is the probability of event $B$ happening. The conditional probability is useful when we are interested in understanding the relationship between two events and how the occurrence of one event affects the probability of another event happening.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">  $P(A \\cap B)$ and $P(A,B)$ represent the same concept, which is the probability of both events A and B occurring together. The difference is that $P(A \\cap B)$ is written using set notation and represents the probability of the intersection of two events, while $P(A,B)$ is written using tuple notation and represents the joint probability of two events.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee8e66",
   "metadata": {},
   "source": [
    "The **Marginal Probability** of a random variable $X$, denoted by $P(X)$, is the probability of a particular value of the variable $X$ happening, marginalized or summed over all possible values of the other variables. For example, if we have a joint distribution $P(X,Y)$, the marginal probability of $X$ can be obtained by summing the joint probabilities over all possible values of $Y$:\n",
    "\n",
    "$P(X) = \\sum_Y P(X,Y)$.\n",
    "\n",
    "Similarly, the marginal probability of $Y$ can be obtained by summing the joint probabilities over all possible values of $X$:\n",
    "\n",
    "$P(Y) = \\sum_X P(X,Y)$.\n",
    "\n",
    "The marginal probability is useful when we are interested in understanding the probability distribution of a single variable, without considering the relationship with other variables.\n",
    "\n",
    "Marginal probability refers to the probability of a single event occurring in a multi-event scenario. It can be thought of as the sum of the joint probabilities for a specific event, taking into account all possible outcomes of the other events. For example, consider a scenario where we have two random variables X and Y. The joint probability distribution of X and Y is given by $P(X=x_i,Y=y_j)$, where $x_i$ and $y_j$ are the possible outcomes for X and Y, respectively.\n",
    "\n",
    "It's important to note that marginal probabilities must also satisfy the non-negativity and normalization properties of probability distributions.\n",
    "\n",
    "\n",
    "In Python, these forms of probability distributions can be implemented using numpy arrays and basic mathematical operations. For example, the conditional probability of event $A$ given event $B$ can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b8fa9e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.503280Z",
     "start_time": "2023-02-25T13:37:21.489282Z"
    }
   },
   "outputs": [],
   "source": [
    "def conditional_probability(B, joint_distribution):\n",
    "    \"\"\"\n",
    "    Calculates the conditional probability of A given B, P(A|B), from the joint probability distribution.\n",
    "\n",
    "    Parameters:\n",
    "        B (int): Index of the random variable given in the joint distribution.\n",
    "        joint_distribution (np.ndarray): 2D array representing the joint probability distribution of the two random variables.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 1D array representing the conditional probability of A given B.\n",
    "    \"\"\"\n",
    "    p_x = np.sum(joint_distribution, axis=B)\n",
    "    #print(p_b)\n",
    "    p_Y_given_x = joint_distribution/ p_x\n",
    "    return p_Y_given_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a78c8",
   "metadata": {},
   "source": [
    "And the marginal probability can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e48ff3c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.519239Z",
     "start_time": "2023-02-25T13:37:21.505239Z"
    }
   },
   "outputs": [],
   "source": [
    "def marginal_probability(X, joint_distribution):\n",
    "    \"\"\"\n",
    "    Calculates the marginal probability of a single random variable X from the joint probability distribution.\n",
    "\n",
    "    Parameters:\n",
    "        X (int): Index of the random variable in the joint distribution.\n",
    "        joint_distribution (np.ndarray): 2D array representing the joint probability distribution of two random variables.\n",
    "\n",
    "    Returns:\n",
    "        float: Marginal probability of the random variable X.\n",
    "    \"\"\"\n",
    "    p_x = np.sum(joint_distribution, axis=X)\n",
    "    return p_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8eb2e",
   "metadata": {},
   "source": [
    "The conditional_probability function takes three inputs: the indices of the two random variables A and B in the joint distribution, and the joint distribution represented as a 2D numpy array joint_distribution. The function first calculates the marginal probability of the second random variable B by summing the joint distribution along the second axis (columns). Then, it calculates the conditional probability of A given B by dividing the values in the row of the joint distribution corresponding to A by the marginal probability of B. The result is a 1D numpy array representing the conditional probability of A given B.\n",
    "\n",
    "The marginal_probability function takes two inputs: the index of the random variable X in the joint distribution and the joint distribution represented as a 2D numpy array joint_distribution. The function calculates the marginal probability of X by summing the joint distribution along the first axis (rows). The result is a scalar representing the marginal probability of X.\n",
    "\n",
    "An now, for something completely different: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "543238f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.535158Z",
     "start_time": "2023-02-25T13:37:21.523189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probability of A given B: [[0.6 0.4]\n",
      " [0.2 0.8]]\n",
      "Marginal probability of X: [0.4 0.6]\n"
     ]
    }
   ],
   "source": [
    "# Example joint distribution\n",
    "joint_distribution = np.array([[0.3, 0.2], [0.1, 0.4]])\n",
    "\n",
    "# Example calculation of conditional probability\n",
    "p_a_given_b = conditional_probability(1, joint_distribution)\n",
    "print(\"Conditional probability of A given B:\", p_a_given_b)\n",
    "\n",
    "# Example calculation of marginal probability\n",
    "p_x = marginal_probability(0, joint_distribution)\n",
    "print(\"Marginal probability of X:\", p_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5301940b",
   "metadata": {},
   "source": [
    "Besides the joint probability distribution, the conditional probability and marginal probability are important forms of probability functions that are applied to sets of two or more variables in classical information theory. They provide valuable insights into the relationship between variables and the probability distribution of individual variables, and can be easily implemented using numpy arrays and basic mathematical operations in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa36e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T14:09:25.276713Z",
     "start_time": "2023-02-08T14:08:47.229042Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 1:</b> Given the conditional probability of two variables, is it possible to recover the joint probability? How?\n",
    "</div>\n",
    "\n",
    "\n",
    "Yes, given the conditional probability, it is possible to recover the joint probability by using the following formula:\n",
    "\n",
    "$P(X = x, Y = y) = P(Y = y | X = x) \\cdot P(X = x)$\n",
    "\n",
    "where $P(X = x, Y = y)$ is the joint probability of the events $X = x$ and $Y = y$, $P(Y = y | X = x)$ is the conditional probability of the event $Y = y$ given the event $X = x$, and $P(X = x)$ is the marginal probability of the event $X = x$. This formula says that the joint probability of two events can be calculated as the product of the conditional probability of one event given the other and the marginal probability of the other event.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 2:</b> Given the marginal probability, is it possible to infer any information about the joint and conditional probability distributions of two random variables?\n",
    "</div>\n",
    "\n",
    "From the marginal probability, it is possible to get some limited information about the joint and conditional probabilities of two random variables. The marginal probability represents the probability of one random variable without considering the other. For example, consider two random variables $X$ and $Y$ and their joint probability distribution $P(X, Y)$. The marginal probability of $X$ is defined as:\n",
    "\n",
    "$P_X(x) = \\sum_{y \\in Y} P(X=x, Y=y)$\n",
    "\n",
    "and the marginal probability of $Y$ is defined as:\n",
    "\n",
    "$P_Y(y) = \\sum_{x \\in X} P(X=x, Y=y)$\n",
    "\n",
    "From the marginal probability, it is possible to compute the expected value of the random variables, but it is not possible to determine their joint distribution or their conditional distributions. To obtain the joint and conditional distributions, additional information is required, such as the joint or conditional probability mass or density functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4facfa94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T14:09:25.276713Z",
     "start_time": "2023-02-08T14:08:47.229042Z"
    }
   },
   "source": [
    "#### Ensembles for multivariable probability distributions\n",
    "\n",
    "So far we have represented joint distribution of two variables with the help of a rank-2 array, a matrix if you like, say:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "0.1 & 0.3 & 0.2 \\\\\n",
    "0.2 & 0.2 & 0.3 \\\\\n",
    "0.3 & 0.1 & 0.2 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "This approach could be extended to joint distributions of more variable by increasing the rank of the array, but this is not always the most practical. Here is an alternative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d65a299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.551114Z",
     "start_time": "2023-02-25T13:37:21.537153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('A', 1, 'Yes'): 0.08333333333333333, ('A', 1, 'No'): 0.08333333333333333, ('A', 2, 'Yes'): 0.08333333333333333, ('A', 2, 'No'): 0.08333333333333333, ('B', 1, 'Yes'): 0.08333333333333333, ('B', 1, 'No'): 0.08333333333333333, ('B', 2, 'Yes'): 0.08333333333333333, ('B', 2, 'No'): 0.08333333333333333, ('C', 1, 'Yes'): 0.08333333333333333, ('C', 1, 'No'): 0.08333333333333333, ('C', 2, 'Yes'): 0.08333333333333333, ('C', 2, 'No'): 0.08333333333333333}\n"
     ]
    }
   ],
   "source": [
    "X = ['A', 'B', 'C']    # possible values of the first random variable\n",
    "Y = [1, 2]             # possible values of the second random variable\n",
    "Z = ['Yes', 'No']      # possible values of the third random variable\n",
    "\n",
    "# create a dictionary to represent the output space\n",
    "output_space = {}\n",
    "for x in X:\n",
    "    for y in Y:\n",
    "        for z in Z:\n",
    "            # assign a probability of 1/12 to each possible combination of outcomes\n",
    "            output_space[(x,y,z)] = 1/12\n",
    "\n",
    "# print the output space\n",
    "print(output_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36bc447",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T14:09:25.276713Z",
     "start_time": "2023-02-08T14:08:47.229042Z"
    }
   },
   "source": [
    "This code defines a three-partite random variable with three possible values for the first variable ('A', 'B', and 'C'), two possible values for the second variable (1 and 2), and two possible values for the third variable ('Yes' and 'No'). The output space is represented as a dictionary, where each key is a tuple representing a possible combination of outcomes, and each value is the corresponding probability. In this case, each possible combination of outcomes has a probability of 1/12, since there are 322=12 possible outcomes in total.\n",
    "\n",
    "This type of structure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f07e9",
   "metadata": {},
   "source": [
    "## Quantifying information\n",
    "\n",
    "In a probability distribution, there are several quantities that can be used to quantify and characterize the information. These quantities include entropy, mutual information, Kullback-Leibler divergence, and cross-entropy.\n",
    "\n",
    "The entropy of a probability distribution is a measure of the amount of uncertainty associated with a random variable. It is defined as:\n",
    "\n",
    "$$H(X) = -\\sum_{x_i\\in A_X} p_i \\log p_i$$\n",
    "\n",
    "where $p_i$ is the probability of observing the $i$-th state $x_i$ of the random variable $X$.\n",
    "\n",
    "The mutual information is a measure of the amount of shared information between two random variables. It is defined as:\n",
    "\n",
    "$$I(X;Y) = \\sum_{x_i\\in A_X} \\sum_{y_j\\in A_Y} p(x_i,y_j)\\log \\left(\\frac{p(x_i,y_j)}{p_i p_j}\\right)$$\n",
    "\n",
    "where $p(x_i,y_j)$ is the joint probability of observing states $x_i$ and $y_j$ for random variables $X$ and $Y$, respectively, and $p_i$ and $p_j$ are the marginal probabilities of observing states $x_i$ and $y_j$, respectively.\n",
    "\n",
    "The Kullback-Leibler divergence, also known as the relative entropy, is a measure of the difference between two probability distributions. It is defined as:\n",
    "\n",
    "$$D_{KL}(P||Q) = \\sum_{x_i\\in A_X} p_i \\log \\left(\\frac{p_i}{q_i}\\right)$$\n",
    "\n",
    "where $p_i$ and $q_i$ are the probabilities of observing state $x_i$ for random variables $P$ and $Q$, respectively.\n",
    "\n",
    "The cross-entropy is a measure of the difference between a true probability distribution and an estimated probability distribution. It is defined as:\n",
    "\n",
    "$$H(P,Q) = -\\sum_{x_i\\in A_X} p_i \\log q_i$$\n",
    "\n",
    "where $p_i$ is the true probability of observing state $x_i$ for random variable $P$, and $q_i$ is the estimated probability of observing state $x_i$ for random variable $Q$.\n",
    "\n",
    "These quantities can be used to describe and analyze the information content of a probability distribution, and they play an important role in many applications of classical information theory.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "Entropy is a measure of uncertainty in a random variable. In other words, it quantifies the amount of information required to specify the state of the random variable. \n",
    "\n",
    "The entropy of a discrete random variable with states $x_1, x_2, \\ldots, x_n$ and probabilities $p_1, p_2, \\ldots, p_n$ is given by:\n",
    "\n",
    "$$ H(X) = - \\sum_{i=1}^n p_i \\log_2 p_i $$\n",
    "\n",
    "Let's try to calculate the entropy of a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97609ed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.567111Z",
     "start_time": "2023-02-25T13:37:21.553109Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of the random variable is 0.81 bits\n"
     ]
    }
   ],
   "source": [
    "def entropy(p):\n",
    "    \"\"\"\n",
    "    Calculates the entropy of a probability distribution p\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : numpy array\n",
    "        Probability distribution\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    h : float\n",
    "        Entropy of the distribution\n",
    "    \"\"\"\n",
    "    h = - np.sum(p * np.log2(p))\n",
    "    return h\n",
    "\n",
    "\n",
    "#use example\n",
    "\n",
    "p = np.array([0.25, 0.75])\n",
    "ent = entropy(p)\n",
    "print(\"The entropy of the random variable is {:.2f} bits\".format(ent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dba59b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Problem 1:** *Entropy of a classical random variable.*\n",
    "\n",
    "\n",
    "Suppose we have a random variable X that takes on two possible values, 0 and 1, with equal probability. What is the entropy of X?\n",
    "\n",
    "*Solution:*\n",
    "\n",
    "The entropy of a random variable X is defined as:\n",
    "\n",
    "$$H(X) = -\\sum_{x \\in \\text{outcomes}} P(x)\\log_2 P(x)$$\n",
    "\n",
    "For this example, the two possible outcomes are 0 and 1, and each has a probability of 0.5. Hence,\n",
    "\n",
    "$$H(X) = -\\left(0.5\\log_2 0.5 + 0.5\\log_2 0.5\\right) = -(0.5 \\cdot -1 + 0.5 \\cdot -1) = 1$$\n",
    "\n",
    "So the entropy of the random variable X is 1 bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163ddec8",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "\n",
    "Mutual information is a measure of the amount of shared information between two random variables. It quantifies the reduction in uncertainty of one random variable given the knowledge of another. \n",
    "\n",
    "The mutual information between two discrete random variables $X$ and $Y$ with joint probability mass function $p_{X,Y}(x,y)$ is given by:\n",
    "\n",
    "$$ I(X;Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p_{X,Y}(x,y) \\log_2 \\frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)} $$\n",
    "\n",
    "Let's try to calculate the mutual information between two simple examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "526f1793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.583060Z",
     "start_time": "2023-02-25T13:37:21.569107Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mutual entropy between X and Y is: 0.26 bits\n"
     ]
    }
   ],
   "source": [
    "def mutual_entropy(p_xy):\n",
    "    \"\"\"\n",
    "    Calculates the mutual entropy between two random variables X and Y\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p_xy : numpy array\n",
    "        Joint probability distribution of X and Y\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    h_xy : float\n",
    "        Mutual entropy between X and Y\n",
    "    \"\"\"\n",
    "    p_x = np.sum(p_xy, axis=1)\n",
    "    p_y = np.sum(p_xy, axis=0)\n",
    "    \n",
    "    h_x = entropy(p_x)\n",
    "    h_y = entropy(p_y)\n",
    "    h_xy = entropy(p_xy.flatten())\n",
    "    \n",
    "    h_xy = h_x + h_y - h_xy\n",
    "    \n",
    "    return h_xy\n",
    "\n",
    "p_xy = np.array([[0.3, 0.1], [0.1, 0.5]])\n",
    "mutual_entropy = mutual_entropy(p_xy)\n",
    "\n",
    "print(f\"The mutual entropy between X and Y is: {mutual_entropy:.2f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0bd355",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Problem 2:** *Mutual information between two classical random variables.*\n",
    "\n",
    "Suppose we have two random variables X and Y. X takes on two possible values, 0 and 1, with equal probability, and Y takes on two possible values, 0 and 1, with probabilities 0.7 and 0.3 respectively. What is the mutual entropy between X and Y?\n",
    "\n",
    "Solution:\n",
    "The mutual entropy between two random variables X and Y is defined as:\n",
    "\n",
    "$$H(X,Y) = -\\sum_{x \\in \\text{outcomes of X}} \\sum_{y \\in \\text{outcomes of Y}} P(x,y)\\log_2 P(x,y)$$\n",
    "\n",
    "We can calculate the joint probability of each combination of outcomes (0,0), (0,1), (1,0), and (1,1) and then plug it into the above formula. For example,\n",
    "\n",
    "$$P(0,0) = P(X=0)\\cdot P(Y=0) = 0.5 \\cdot 0.7 = 0.35$$\n",
    "\n",
    "Similarly, the other joint probabilities can be calculated as:\n",
    "\n",
    "$$P(0,1) = 0.5 \\cdot 0.3 = 0.15$$\n",
    "$$P(1,0) = 0.5 \\cdot 0.7 = 0.35$$\n",
    "$$P(1,1) = 0.5 \\cdot 0.3 = 0.15$$\n",
    "\n",
    "Plugging these values into the formula for mutual entropy, we get:\n",
    "\n",
    "$$H(X,Y) = -\\left(0.35\\log_2 0.35 + 0.15\\log_2 0.15 + 0.35\\log_2 0.35 + 0.15\\log_2 0.15\\right)$$\n",
    "$$H(X,Y) = -(0.35 \\cdot -1.67 + 0.15 \\cdot -2.97 + 0.35 \\cdot -1.67 + 0.15 \\cdot -2.97)$$\n",
    "$$H(X,Y) = 1.51$$\n",
    "\n",
    "So the mutual entropy between X and Y is 1.51 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb4937",
   "metadata": {},
   "source": [
    "### Conditional Information\n",
    "\n",
    "Conditional information is a measure of the amount of information that can be obtained about a random variable given the knowledge of another random variable. It is a fundamental concept in classical information theory that is closely related to entropy and mutual entropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65e30e2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.599025Z",
     "start_time": "2023-02-25T13:37:21.584028Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def conditional_entropy(p_xy):\n",
    "    \"\"\"\n",
    "    Calculates the conditional entropy of X given Y for a joint probability distribution of X and Y\n",
    "    Parameters\n",
    "    ----------\n",
    "    p_xy : numpy array\n",
    "    Joint probability distribution of X and Y\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    h_x_given_y : float\n",
    "    Conditional entropy of X given Y\n",
    "    \"\"\"\n",
    "    p_y = np.sum(p_xy, axis=0)\n",
    "    p_x_given_y = np.divide(p_xy, p_y, out=np.zeros_like(p_xy), where=p_y!=0)\n",
    "    h_x_given_y = np.sum(np.multiply(p_xy, np.log2(p_x_given_y)), axis=0)\n",
    "\n",
    "    return - h_x_given_y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb747f3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Problem 3:** Consider two random variables X and Y, with joint probability distribution $P_{X,Y}$. \n",
    "Calculate the conditional entropy of Y given X for the following joint probability distribution:\n",
    "$$\\begin{bmatrix}\n",
    "0.1 & 0.3 & 0.2 \\\\\n",
    "0.2 & 0.2 & 0.3 \\\\\n",
    "0.3 & 0.1 & 0.2 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Solution: \n",
    "\n",
    "First, let's calculate the conditional probability $P_{Y|X}(y|x)$:\n",
    "$$P_{Y|X}(y|x_1) = \\frac{P_{X,Y}(x_1,y)}{P_X(x_1)} = \\frac{[0.1, 0.3, 0.2][y]}{0.1 + 0.3 + 0.2} = [0.125, 0.375, 0.5]$$\n",
    "$$P_{Y|X}(y|x_2) = \\frac{P_{X,Y}(x_2,y)}{P_X(x_2)} = \\frac{[0.2, 0.2, 0.3][y]}{0.2 + 0.2 + 0.3} = [0.25, 0.25, 0.5]$$\n",
    "$$P_{Y|X}(y|x_3) = \\frac{P_{X,Y}(x_3,y)}{P_X(x_3)} = \\frac{[0.3, 0.1, 0.2][y]}{0.3 + 0.1 + 0.2} = [0.6, 0.2, 0.2]$$\n",
    "Next, we can use these conditional probabilities to calculate the conditional entropy:\n",
    "$$H(Y|X) = -\\sum_{x\\in X} \\sum_{y\\in Y} P_{X,Y}(x,y) \\log P_{Y|X}(y|x)$$\n",
    "$$H(Y|X) = -[0.1 \\times \\log(0.125) + 0.3 \\times \\log(0.375) + 0.2 \\times \\log(0.5)] - [0.2 \\times \\log(0.25) + 0.2 \\times \\log(0.25) + 0.3 \\times \\log(0.5)] - [0.3 \\times \\log(0.6) + 0.1 \\times \\log(0.2) + 0.2 \\times \\log(0.2)]$$\n",
    "$$H(Y|X) = 0.276$$\n",
    "So, the conditional entropy of Y given X is approximately 0.276."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4456f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "\n",
    "Congratulations! You have just taken your first steps towards understanding some of the foundational concepts of modern information theory. The study of probability and classical information theory provides us with the tools to quantify and understand uncertainty, randomness, and information in the world around us. By learning about ensembles, joint and marginal probabilities, and conditional probability, you have gained a deeper understanding of how information is stored and transmitted.\n",
    "\n",
    "As you continue your journey into the world of quantum information theory, you may have many questions in your mind. What is the relationship between classical information theory and quantum information theory? How do quantum mechanics influence the way we think about information? What are some of the unique challenges and opportunities that quantum information theory presents?\n",
    "\n",
    "As you delve deeper into these topics, keep in mind the importance of the concepts you have learned. Probability theory provides the foundation for much of modern information theory, and it is crucial to have a strong understanding of these concepts in order to make progress in the field. Take these building blocks, and use them as a foundation for your future studies. Good luck!\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Question 3:</b> So, when you are manipulating information, are you working with distribution functions? I mean, when you send a Morse encoded message through a channel your are not sending the probability that the next letter is a \"X\" or a \"Z\". You are actually sending a specific letter and the receiver may receive, not receive, or receive mistakenly. How do you translate these situations into probability distributions? Provide your views on this question. \n",
    "</div>   \n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 4:</b> In your own words, provide definitions and contextualization of the following concepts:\n",
    "ensemble, probability vector, probability, joint probability, conditional probability, entropy, mutual information, relative entropy and inference. \n",
    "</div> \n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 5:</b> Underlying the concepts presented in this notebook is the notion of probability distribution of a random variable, whether that variable is a physical state or the result of a measurement, say $p(x)$. In future notebooks we shall use the notion of wave vector. Discuss what you expect a wave vector to be, what properties should it have and how do they differ from a probability vector? \n",
    "</div>\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 6:</b> Given that wave vectors and probability vectors are distinct, how would you combine the the two objects into a single one combining the informational content of probability vector and the capacity to predict the probability of measurements of a wave vector?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 7:</b> If you have a physics background then, you must be recognizing many concepts from statistical mechanics in classical information theory. Can you extrapolate on which other concepts can be adopted in information theory from statistical physics?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question 8:</b> Let now think the other way around. What concepts of information theory do you think can be borrowed by physics, specially quantum physics of complex systems? This question in not about providing a correct answer but rather to stimulate your brain muscles beyond the technical aspects. Let your mind fly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1208c4f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**EXERCISES:**\n",
    "    \n",
    "Today's exercises are probably about probabilities in classical information theory but trying to bridge to the quantum counterpart. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Exercise 1:** *Calculation of entropy, mutual entropy and conditional information*\n",
    "\n",
    "A system consists of two random variables, X and Y, with joint probability distribution given by the following table:\n",
    "\n",
    "| X / Y | 0   | 1   | 2   |\n",
    "|-------|-----|-----|-----|\n",
    "| 0     | 0.1 | 0.2 | 0.1 |\n",
    "| 1     | 0.1 | 0.3 | 0.2 |\n",
    "| 2     | 0.2 | 0.1 | 0.3 |\n",
    "\n",
    "\n",
    "\n",
    "Calculate the entropy of X, entropy of Y, and the mutual entropy of X and Y.\n",
    "\n",
    "\n",
    "\n",
    "**Exercise 2:** *Advanced entropy and mutual entropy calculations*\n",
    "\n",
    "Consider a discrete random variable X with the following probability distribution:\n",
    "\n",
    "$$P(X = 0) = \\frac{1}{3}$$\n",
    "$$P(X = 1) = \\frac{2}{3}$$\n",
    "\n",
    "and another discrete random variable Y with the following probability distribution:\n",
    "\n",
    "$$P(Y = 0) = \\frac{1}{2}$$\n",
    "$$P(Y = 1) = \\frac{1}{2}$$\n",
    "\n",
    "Calculate the entropy of X, entropy of Y, the joint entropy of X and Y, and the conditional entropy of X given Y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41fdf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 4:** *Calculation of Marginal and Conditional Probabilities*\n",
    "\n",
    "Suppose we have a joint probability distribution $P(X, Y)$ with the following values:\n",
    "\n",
    "$$ P(X = 0, Y = 0) = 0.2$$\n",
    "$$ P(X = 0, Y = 1) = 0.1$$\n",
    "$$ P(X = 1, Y = 0) = 0.3$$\n",
    "$$ P(X = 1, Y = 1) = 0.15$$\n",
    "$$ P(X = 2, Y = 0) = 0.15$$\n",
    "$$ P(X = 2, Y = 1) = 0.05$$\n",
    "\n",
    "Task: Calculate the marginal probability distributions $P(X)$ and $P(Y)$ and the conditional probability distributions $P(X|Y)$ and $P(Y|X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec40a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 5:** *Calculating Joint Probabilities*\n",
    "\n",
    "Suppose you have two random variables X and Y, where X can take on one of two values, 0 or 1, and Y can take on one of three values, 0, 1, or 2. The joint probability mass function of X and Y is given as follows:\n",
    "\n",
    "\n",
    "$$P(X = 0, Y = 0) = 0.1$$\n",
    "$$P(X = 0, Y = 1) = 0.2$$\n",
    "$$P(X = 0, Y = 2) = 0.05$$\n",
    "$$P(X = 1, Y = 0) = 0.05$$\n",
    "$$P(X = 1, Y = 1) = 0.1$$\n",
    "$$P(X = 1, Y = 2) = 0.15$$\n",
    "\n",
    "\n",
    "Task: Write a Python code to calculate the joint probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a0927",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 6:** *Calculation of Hypothesis Probability*\n",
    "\n",
    "Suppose we have a set of random variables $X$, $Y$, and $Z$, where $X$ can take on values from ${1, 2, 3}$, $Y$ can take on values from ${1, 2, 3, 4}$, and $Z$ can take on values from ${0, 1}$. We also have a joint probability mass function (PMF) $P(X, Y, Z)$ defined as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "P(X = 1, Y = 1, Z = 0) &= 0.05 \\\\\n",
    "P(X = 1, Y = 2, Z = 0) &= 0.1 \\\\\n",
    "P(X = 1, Y = 2, Z = 1) &= 0.1 \\\\\n",
    "P(X = 1, Y = 3, Z = 0) &= 0.05 \\\\\n",
    "P(X = 2, Y = 1, Z = 1) &= 0.1 \\\\\n",
    "P(X = 2, Y = 2, Z = 0) &= 0.05 \\\\\n",
    "P(X = 2, Y = 3, Z = 0) &= 0.1 \\\\\n",
    "P(X = 2, Y = 4, Z = 1) &= 0.05 \\\\\n",
    "P(X = 3, Y = 1, Z = 0) &= 0.1 \\\\\n",
    "P(X = 3, Y = 3, Z = 1) &= 0.05 \\\\\n",
    "P(X = 3, Y = 4, Z = 0) &= 0.05\n",
    "\\end{align*}\n",
    "\n",
    "The exercise is to calculate the probability of the hypothesis $H = {(X, Y, Z) : X = 1, Y = 2, Z = 1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bab32a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 6:** *Calculation of Hypothesis Probability*\n",
    "\n",
    "Suppose we have a set of random variables $X$, $Y$, and $Z$, where $X$ can take on values from ${1, 2, 3}$, $Y$ can take on values from ${1, 2, 3, 4}$, and $Z$ can take on values from ${0, 1}$. We also have a joint probability mass function (PMF) $P(X, Y, Z)$ defined as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "P(X = 1, Y = 1, Z = 0) &= 0.05 \\\\\n",
    "P(X = 1, Y = 2, Z = 0) &= 0.1 \\\\\n",
    "P(X = 1, Y = 2, Z = 1) &= 0.1 \\\\\n",
    "P(X = 1, Y = 3, Z = 0) &= 0.05 \\\\\n",
    "P(X = 2, Y = 1, Z = 1) &= 0.1 \\\\\n",
    "P(X = 2, Y = 2, Z = 0) &= 0.05 \\\\\n",
    "P(X = 2, Y = 3, Z = 0) &= 0.1 \\\\\n",
    "P(X = 2, Y = 4, Z = 1) &= 0.05 \\\\\n",
    "P(X = 3, Y = 1, Z = 0) &= 0.1 \\\\\n",
    "P(X = 3, Y = 3, Z = 1) &= 0.05 \\\\\n",
    "P(X = 3, Y = 4, Z = 0) &= 0.05\n",
    "\\end{align*}\n",
    "\n",
    "The exercise is to calculate the probability of the hypothesis $H = {(X, Y, Z) : X = 1, Y = 2, Z = 1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682c186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 7:** *Hypothesis testing using the Bayes' theorem*\n",
    "\n",
    "A disease is affecting a certain population. The probability of getting the disease is 1%. A diagnostic test for the disease is performed and is known to be 90% accurate. That means, if a person has the disease, the test will return positive result in 90% of the cases, and if a person does not have the disease, the test will return negative result in 90% of the cases.\n",
    "\n",
    "A person goes through the test and the result is positive. What is the probability that the person has the disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95b3b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 8:** *Using the Bayes' theorem to update the probability of a hypothesis in the context of classical communications*\n",
    "\n",
    "Consider a binary communication channel with a Bernoulli input distribution, where a binary data symbol \"0\" or \"1\" is transmitted over the channel with probability $p$ and $q = 1 - p$, respectively. Suppose the channel is modeled as a binary symmetric channel (BSC), meaning that the received symbol is different from the transmitted symbol with probability $e$.\n",
    "\n",
    "Suppose we transmit the symbol \"0\" and receive the symbol \"1\". We want to calculate the probability that the transmitted symbol was \"0\" given that the received symbol is \"1\". This is equivalent to finding the probability $P(X=0|Y=1)$, where $X$ is the transmitted symbol and $Y$ is the received symbol.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b321582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 9:** *Maximum Likelihood Inference*\n",
    "\n",
    "Consider a communication channel where the transmission of bits is modeled as a Bernoulli distribution. In this channel, if a 1 bit is transmitted, the receiver gets a 1 with probability p, and if a 0 bit is transmitted, the receiver gets a 0 with probability q. Let X1, X2, ..., Xn be the transmitted bits and Y1, Y2, ..., Yn be the received bits.\n",
    "\n",
    "Suppose we want to estimate the probability of receiving a 1 given that a 1 was transmitted, denoted as p. We can do this by using the maximum likelihood method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d35503",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 10:** *Maximum Likelihood Inference*\n",
    "\n",
    "Given the observed received bits Y = [1, 1, 0, 1, 0, 0, 1], find the maximum likelihood estimate for p.\n",
    "Solution:\n",
    "\n",
    "The likelihood function is given by:\n",
    "\n",
    "$$L(p) = p^k(1-p)^{n-k}$$\n",
    "\n",
    "where k is the number of ones observed in the received bits and n is the total number of bits transmitted.\n",
    "\n",
    "The goal is to find the value of p that maximizes the likelihood function:\n",
    "\n",
    "$$p_{ML} = \\arg\\max_p L(p)$$\n",
    "\n",
    "$$p_{ML} = \\frac{k}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb2753",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 11:** *Bayesian Inference*\n",
    "\n",
    "A box contains 4 balls, of which 3 are red and 1 is blue. A ball is picked from the box randomly and is not replaced. The person then picks a second ball. What is the probability that both balls are blue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1563a1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 12:** *Inferring the parameters of a Poisson distribution**\n",
    "\n",
    "In this exercise, you will use maximum likelihood estimation and Bayesian inference to infer the parameter of a Poisson distribution.\n",
    "\n",
    "Suppose that the number of events in a given time period follows a Poisson distribution with an unknown parameter $\\lambda$. The likelihood function for a sample of $n$ events is given by:\n",
    "\n",
    "$$L(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{x_i}}{x_i!}$$\n",
    "\n",
    "where $x_i$ is the number of events observed in the $i$th time period.\n",
    "\n",
    "Find the maximum likelihood estimate of $\\lambda$ by taking the derivative of the log-likelihood function with respect to $\\lambda$, setting it to zero, and solving for $\\lambda$.\n",
    "\n",
    "Write a python function to perform Bayesian inference for $\\lambda$ using a uniform prior. Draw samples from the posterior distribution using Markov Chain Monte Carlo (MCMC) and compare the results to the maximum likelihood estimate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8ba6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 13:** *Discrete Variables Inference*\n",
    "\n",
    "Consider a discrete random variable X that can take values {0,1,2,3} with probabilities {0.1, 0.3, 0.3, 0.3}. Suppose that we observe X=2 and we want to update the probability of X given this observation.\n",
    "\n",
    "Calculate the maximum likelihood estimate (MLE) of X given the observation X=2.\n",
    "Calculate the Bayesian estimate of X given the observation X=2 and a prior probability {0.05, 0.2, 0.4, 0.35}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec86fcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 14:** *Quantum State Discrimination*\n",
    "\n",
    "Consider an unknown quantum gate that transforms an input state X into an output state Y. The input state X is a quantum state in the set of states for a two-level system, and the probability of sending X through the gate is known, given by p(X). The output state Y is not known to the observer. The observer measures the observable operator Mz = |1><1|-|0><0|, yielding a random variable Z. The experiment is performed several times to obtain the frequency probability distribution p(z) at the arrival.\n",
    "\n",
    "Task:\n",
    "\n",
    "Define the joint probability distribution of X, Y, and Z.\n",
    "Calculate the marginal probability distributions of X and Z.\n",
    "Calculate the conditional probability distribution of X given Z.\n",
    "Compare the results of the marginal and conditional probability distributions with the frequency probability distribution obtained from the experiment.\n",
    "\n",
    "\n",
    "Note: This exercise can be extended to consider more complex quantum systems, such as multi-level systems, and to explore the concepts of entropy, mutual entropy, and information gain in quantum systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff27d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "**Exercise 15:** *The black box problem*\n",
    "\n",
    "This is an interesting exercise! Here is a code for the \"black-box\" function in Python that generates the random quantum states and applies either the \"not gate\" or the \"identity gate\" to it, and measures the resulting state. This function can be used to test different methods of inference to determine the type of gate that was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "167ccbe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-25T13:37:21.614945Z",
     "start_time": "2023-02-25T13:37:21.601007Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantum_black_box(n, distribution):\n",
    "    \"\"\"\n",
    "    This function generates n instantiations of a random variable X, corresponding to a quantum state of a two level system.\n",
    "    The superposition coefficients are generated following the given distribution. The function then applies either the\n",
    "    \"not gate\" or the \"identity gate\" to the state X and measures the resulting state.\n",
    "    :param n: number of instantiations\n",
    "    :param distribution: a list of the form [p0, p1] containing the probabilities for the superposition coefficients\n",
    "    :return: an array of n measurements\n",
    "    \"\"\"\n",
    "    results = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        # Generate the superposition coefficients based on the provided distribution\n",
    "        p0, p1 = distribution\n",
    "        prob = random.uniform(0, 1)\n",
    "        if prob <= p0:\n",
    "            alpha = np.sqrt(p0)\n",
    "            beta = np.sqrt(1 - p0)\n",
    "        else:\n",
    "            alpha = np.sqrt(p1)\n",
    "            beta = np.sqrt(1 - p1)\n",
    "        \n",
    "        # Choose either the \"not gate\" or the \"identity gate\" to apply to the state X\n",
    "        gate = random.choice([np.array([[0, 1],[1, 0]]), np.identity(2)])\n",
    "        \n",
    "        # Calculate the resulting state Y\n",
    "        state = np.array([alpha, beta])\n",
    "        state = np.matmul(gate, state)\n",
    "        \n",
    "        # Measure the resulting state using the observable Mz\n",
    "        prob_0 = np.abs(state[0])**2\n",
    "        prob_1 = np.abs(state[1])**2\n",
    "        if random.uniform(0, 1) <= prob_0:\n",
    "            results[i] = 0\n",
    "        else:\n",
    "            results[i] = 1\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01845050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "The exercise can be extended by considering more complex quantum states, different quantum channels, or exploring the relationship between the entropy difference and the noise in the channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abfdb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T17:01:24.291646Z",
     "start_time": "2023-02-08T17:00:39.894891Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Today's afterthoughts:</b>\n",
    "\n",
    "Reflect on the meaning of classical information as a probability distribution or as an entropy. Can you think of limitations to these approaches? Why do you think that quantum information is not just applying classical information to random variables identified with quantum states? What should be missing, if anything, in a classical information theory about quantum states to become a full quantum theory?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b94eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
